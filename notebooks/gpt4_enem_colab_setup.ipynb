{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ GPT-4-ENEM no Google Colab\n",
        "\n",
        "Este notebook configura o ambiente para avaliar modelos de linguagem no ENEM usando GPUs do Google Colab (A100, T4, V100, etc.)\n",
        "\n",
        "## üìã Pr√©-requisitos\n",
        "- Conta Google (para acessar Colab)\n",
        "- Chave API da Maritaca (ou OpenAI)\n",
        "- Acesso a GPU (gratuito ou Colab Pro)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Configurar GPU\n",
        "\n",
        "**IMPORTANTE**: V√° em **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**\n",
        "\n",
        "Para A100: Requer **Colab Pro** ou **Colab Pro+**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar GPU dispon√≠vel\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Mem√≥ria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU n√£o detectada. Certifique-se de ativar GPU em Runtime ‚Üí Change runtime type\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Instalar Depend√™ncias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar depend√™ncias principais\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers datasets scikit-learn\n",
        "!pip install -q sqlitedict pytablewriter sacrebleu rouge-score\n",
        "!pip install -q pycountry numexpr tqdm jsonlines\n",
        "!pip install -q openai fschat\n",
        "!pip install -q git+https://github.com/lm-sys/FastChat.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clonar reposit√≥rio (ou fazer upload manual)\n",
        "import os\n",
        "\n",
        "# Op√ß√£o 1: Clonar do GitHub\n",
        "!git clone https://github.com/piresramon/gpt-4-enem.git\n",
        "os.chdir('gpt-4-enem')\n",
        "\n",
        "# Op√ß√£o 2: Se j√° tiver os arquivos, fazer upload manual via Colab\n",
        "# Files ‚Üí Upload ‚Üí selecionar arquivos do projeto\n",
        "\n",
        "print(\"‚úÖ Reposit√≥rio clonado/configurado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar projeto em modo desenvolvimento\n",
        "!pip install -e .\n",
        "\n",
        "print(\"‚úÖ Projeto instalado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Configurar Chaves API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Configurar chave API da Maritaca\n",
        "# SUBSTITUA pela sua chave real\n",
        "os.environ['CURSORMINIMAC'] = '107341642936117619902_e1ed52697ebc2587'\n",
        "\n",
        "# Alternativamente, use:\n",
        "# os.environ['MARITALK_API_SECRET_KEY'] = 'sua-chave-aqui'\n",
        "\n",
        "# Para OpenAI (se necess√°rio):\n",
        "# os.environ['OPENAI_API_SECRET_KEY'] = 'sk-sua-chave-aqui'\n",
        "\n",
        "print(\"‚úÖ Chaves API configuradas\")\n",
        "print(f\"   Maritaca: {'‚úÖ' if os.environ.get('CURSORMINIMAC') or os.environ.get('MARITALK_API_SECRET_KEY') else '‚ùå'}\")\n",
        "print(f\"   OpenAI: {'‚úÖ' if os.environ.get('OPENAI_API_SECRET_KEY') else '‚ùå (opcional)'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testar conex√£o com API Maritaca\n",
        "import openai\n",
        "\n",
        "api_key = os.environ.get('CURSORMINIMAC') or os.environ.get('MARITALK_API_SECRET_KEY')\n",
        "\n",
        "if api_key:\n",
        "    openai.api_base = \"https://chat.maritaca.ai/api\"\n",
        "    \n",
        "    # Detectar vers√£o do openai\n",
        "    openai_version = openai.__version__\n",
        "    major_version = int(openai_version.split('.')[0])\n",
        "    \n",
        "    if major_version >= 1:\n",
        "        client = openai.OpenAI(api_key=api_key, base_url=\"https://chat.maritaca.ai/api\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"sabia-3\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Responda apenas: OK\"}],\n",
        "            max_tokens=5\n",
        "        )\n",
        "        print(f\"‚úÖ API funcionando! Resposta: {response.choices[0].message.content}\")\n",
        "    else:\n",
        "        openai.api_key = api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"sabia-3\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Responda apenas: OK\"}],\n",
        "            max_tokens=5\n",
        "        )\n",
        "        print(f\"‚úÖ API funcionando! Resposta: {response.choices[0].message.content}\")\n",
        "else:\n",
        "    print(\"‚ùå Chave API n√£o configurada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Verificar Dados ENEM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Verificar se os dados est√£o presentes\n",
        "data_dir = Path('data/enem')\n",
        "\n",
        "if data_dir.exists():\n",
        "    print(\"‚úÖ Diret√≥rio de dados encontrado\")\n",
        "    for file in data_dir.glob('*.jsonl'):\n",
        "        count = sum(1 for _ in open(file))\n",
        "        print(f\"   {file.name}: {count} quest√µes\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Diret√≥rio de dados n√£o encontrado\")\n",
        "    print(\"   Os dados ser√£o baixados automaticamente na primeira execu√ß√£o\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Executar Avalia√ß√£o\n",
        "\n",
        "Agora voc√™ pode executar avalia√ß√µes do ENEM!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo: Avaliar Sabi√°-3 no ENEM 2024 (teste r√°pido com --limit)\n",
        "!python main.py \\\n",
        "    --model maritalk \\\n",
        "    --model_args engine=sabia-3 \\\n",
        "    --tasks enem_cot_2024_blind \\\n",
        "    --description_dict_path description.json \\\n",
        "    --num_fewshot 3 \\\n",
        "    --conversation_template chatgpt \\\n",
        "    --limit 5 \\\n",
        "    --output_path results/teste_rapido.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avalia√ß√£o completa (sem --limit)\n",
        "# ‚ö†Ô∏è  Isso pode levar muito tempo e consumir muitos cr√©ditos da API\n",
        "\n",
        "# !python main.py \\\n",
        "#     --model maritalk \\\n",
        "#     --model_args engine=sabia-3 \\\n",
        "#     --tasks enem_cot_2024_blind \\\n",
        "#     --description_dict_path description.json \\\n",
        "#     --num_fewshot 3 \\\n",
        "#     --conversation_template chatgpt \\\n",
        "#     --output_path results/sabia3_enem2024_completo.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Analisar Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar e visualizar resultados\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Criar diret√≥rio de resultados se n√£o existir\n",
        "Path('results').mkdir(exist_ok=True)\n",
        "\n",
        "# Carregar resultados (ajuste o caminho conforme necess√°rio)\n",
        "result_file = 'results/teste_rapido.json'\n",
        "\n",
        "if Path(result_file).exists():\n",
        "    with open(result_file, 'r', encoding='utf-8') as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"üìä Resultados da Avalia√ß√£o:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    if 'results' in results:\n",
        "        for task_name, task_results in results['results'].items():\n",
        "            print(f\"\\nüìã Tarefa: {task_name}\")\n",
        "            if 'acc' in task_results:\n",
        "                print(f\"   ‚úÖ Acur√°cia geral: {task_results['acc']:.2%}\")\n",
        "            \n",
        "            # √Åreas de conhecimento\n",
        "            areas = {\n",
        "                'languages': 'Linguagens e C√≥digos',\n",
        "                'human-sciences': 'Ci√™ncias Humanas',\n",
        "                'natural-sciences': 'Ci√™ncias da Natureza',\n",
        "                'mathematics': 'Matem√°tica'\n",
        "            }\n",
        "            \n",
        "            for area_key, area_name in areas.items():\n",
        "                if area_key in task_results:\n",
        "                    acuracia = task_results[area_key]\n",
        "                    barra = \"‚ñà\" * int(acuracia * 30)\n",
        "                    print(f\"   ‚Ä¢ {area_name:25s}: {acuracia:6.2%} {barra}\")\n",
        "    else:\n",
        "        print(json.dumps(results, indent=2, ensure_ascii=False))\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado: {result_file}\")\n",
        "    print(\"   Execute uma avalia√ß√£o primeiro!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o gr√°fica dos resultados (opcional)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    \n",
        "    result_file = 'results/teste_rapido.json'\n",
        "    \n",
        "    if Path(result_file).exists():\n",
        "        with open(result_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        \n",
        "        if 'results' in results:\n",
        "            for task_name, task_results in results['results'].items():\n",
        "                areas = {\n",
        "                    'languages': 'Linguagens',\n",
        "                    'human-sciences': 'Humanas',\n",
        "                    'natural-sciences': 'Natureza',\n",
        "                    'mathematics': 'Matem√°tica'\n",
        "                }\n",
        "                \n",
        "                area_names = []\n",
        "                acuracias = []\n",
        "                \n",
        "                for area_key, area_name in areas.items():\n",
        "                    if area_key in task_results:\n",
        "                        area_names.append(area_name)\n",
        "                        acuracias.append(task_results[area_key])\n",
        "                \n",
        "                if area_names:\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    bars = plt.bar(area_names, acuracias, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "                    plt.title(f'Acur√°cia por √Årea - {task_name}', fontsize=14, fontweight='bold')\n",
        "                    plt.ylabel('Acur√°cia (%)', fontsize=12)\n",
        "                    plt.ylim(0, 1)\n",
        "                    plt.grid(axis='y', alpha=0.3)\n",
        "                    \n",
        "                    # Adicionar valores nas barras\n",
        "                    for bar, acc in zip(bars, acuracias):\n",
        "                        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                               f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')\n",
        "                    \n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "except ImportError:\n",
        "    print(\"üí° Instale matplotlib para visualiza√ß√µes: !pip install matplotlib\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Erro na visualiza√ß√£o: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Download dos Resultados\n",
        "\n",
        "Para baixar os resultados, use:\n",
        "- **Files ‚Üí Download** (clique com bot√£o direito no arquivo)\n",
        "- Ou execute a c√©lula abaixo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Listar arquivos de resultados dispon√≠veis\n",
        "result_files = list(Path('results').glob('*.json'))\n",
        "\n",
        "if result_files:\n",
        "    print(\"üìÅ Arquivos de resultados dispon√≠veis:\")\n",
        "    for i, file in enumerate(result_files, 1):\n",
        "        size = file.stat().st_size / 1024  # KB\n",
        "        print(f\"   {i}. {file.name} ({size:.1f} KB)\")\n",
        "    \n",
        "    print(\"\\nüí° Para fazer download:\")\n",
        "    print(\"   1. V√° em Files (√≠cone de pasta √† esquerda)\")\n",
        "    print(\"   2. Navegue at√© results/\")\n",
        "    print(\"   3. Clique com bot√£o direito ‚Üí Download\")\n",
        "    print(\"\\n   Ou descomente a linha abaixo para download autom√°tico:\")\n",
        "    print(\"   # files.download('results/teste_rapido.json')\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Nenhum arquivo de resultados encontrado\")\n",
        "    print(\"   Execute uma avalia√ß√£o primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Dicas e Observa√ß√µes\n",
        "\n",
        "### GPU\n",
        "- **Gratuito**: T4 (15GB RAM)\n",
        "- **Colab Pro**: T4, V100, A100 (40GB RAM)\n",
        "- **Colab Pro+**: A100 priorit√°rio\n",
        "\n",
        "### Limites\n",
        "- Sess√µes gratuitas: ~12 horas\n",
        "- Colab Pro: ~24 horas\n",
        "- Use `--limit` para testes r√°pidos\n",
        "\n",
        "### Custos API\n",
        "- Monitore uso da API Maritaca\n",
        "- Avalia√ß√£o completa pode ser cara\n",
        "- Use cache quando poss√≠vel (`--no_cache` desabilitado)\n",
        "\n",
        "### Performance\n",
        "- API Maritaca n√£o usa GPU local (√© API remota)\n",
        "- GPU √© √∫til para modelos locais (transformers)\n",
        "- Para API, GPU n√£o √© necess√°ria, mas Colab oferece ambiente est√°vel\n",
        "\n",
        "### Backup\n",
        "- ‚ö†Ô∏è **IMPORTANTE**: Fa√ßa download dos resultados regularmente\n",
        "- Dados s√£o tempor√°rios e ser√£o perdidos ao desconectar\n",
        "- Use `--output_path` para salvar resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Exemplos de Uso Avan√ßado\n",
        "\n",
        "### M√∫ltiplas Tarefas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avaliar m√∫ltiplas tarefas de uma vez\n",
        "# !python main.py \\\n",
        "#     --model maritalk \\\n",
        "#     --model_args engine=sabia-3 \\\n",
        "#     --tasks enem_cot_2024_blind,enem_cot_2024_captions \\\n",
        "#     --description_dict_path description.json \\\n",
        "#     --num_fewshot 3 \\\n",
        "#     --conversation_template chatgpt \\\n",
        "#     --output_path results/multiplas_tarefas.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparar Diferentes Configura√ß√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar diferentes n√∫meros de few-shot\n",
        "# for num_fewshot in [0, 1, 3]:\n",
        "#     !python main.py \\\n",
        "#         --model maritalk \\\n",
        "#         --model_args engine=sabia-3 \\\n",
        "#         --tasks enem_cot_2024_blind \\\n",
        "#         --description_dict_path description.json \\\n",
        "#         --num_fewshot {num_fewshot} \\\n",
        "#         --conversation_template chatgpt \\\n",
        "#         --limit 10 \\\n",
        "#         --output_path results/fewshot_{num_fewshot}.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificar Status da Sess√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar informa√ß√µes da sess√£o\n",
        "import psutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üìä Status da Sess√£o Colab\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üïê Hora atual: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üíæ RAM dispon√≠vel: {psutil.virtual_memory().available / 1e9:.2f} GB\")\n",
        "print(f\"üíæ RAM total: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "print(f\"üìÅ Diret√≥rio atual: {os.getcwd()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"üíæ GPU livre: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU n√£o dispon√≠vel\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
