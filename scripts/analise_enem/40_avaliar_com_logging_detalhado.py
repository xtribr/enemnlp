#!/usr/bin/env python3
"""
üìä Avalia√ß√£o com Logging Detalhado - ENEM Matem√°tica

====================================================

Este script executa a avalia√ß√£o do ENEM com logging completo para
identificar exatamente quais quest√µes o modelo acerta e erra.

Funcionalidades:
- Log individual de cada quest√£o
- Resposta do modelo vs gabarito
- An√°lise por t√≥pico e TRI
- Exporta√ß√£o para an√°lise posterior

Uso:
    python 40_avaliar_com_logging_detalhado.py [--area matematica|todas]
    python 40_avaliar_com_logging_detalhado.py --area matematica --limit 10  # teste r√°pido
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime
from pathlib import Path

# Tentar importar depend√™ncias
try:
    from datasets import load_dataset
except ImportError:
    print("‚ùå Erro: datasets n√£o instalado")
    print("   Execute: pip install datasets")
    sys.exit(1)

try:
    import openai
except ImportError:
    print("‚ùå Erro: openai n√£o instalado")
    print("   Execute: pip install openai")
    sys.exit(1)


# =============================================================================
# CONFIGURA√á√ïES
# =============================================================================

# Dados TRI das quest√µes de matem√°tica (ENEM 2024)
TRI_DATA = {
    136: {"TRI": 755.3, "H": "H13", "Nivel": "Muito Dif√≠cil", "Tema": "Grandezas e medidas", "Gab": "C"},
    137: {"TRI": 662.3, "H": "H28", "Nivel": "Intermedi√°rio", "Tema": "Estat√≠stica e probabilidade", "Gab": "E"},
    138: {"TRI": 705.0, "H": "H3", "Nivel": "Intermedi√°rio", "Tema": "N√∫meros e opera√ß√µes", "Gab": "B"},
    139: {"TRI": 550.2, "H": "H26", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "A"},
    140: {"TRI": 660.6, "H": "H4", "Nivel": "Intermedi√°rio", "Tema": "N√∫meros e opera√ß√µes", "Gab": "B"},
    141: {"TRI": 701.9, "H": "H20", "Nivel": "Intermedi√°rio", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "B"},
    142: {"TRI": 661.7, "H": "H2", "Nivel": "Intermedi√°rio", "Tema": "N√∫meros e opera√ß√µes", "Gab": "A"},
    143: {"TRI": 792.0, "H": "H18", "Nivel": "Muito Dif√≠cil", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "D"},
    144: {"TRI": 636.5, "H": "H7", "Nivel": "F√°cil", "Tema": "Geometria", "Gab": "D"},
    145: {"TRI": 613.0, "H": "H8", "Nivel": "F√°cil", "Tema": "Geometria", "Gab": "D"},
    146: {"TRI": 809.9, "H": "H22", "Nivel": "Muito Dif√≠cil", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "B"},
    147: {"TRI": 601.8, "H": "H1", "Nivel": "F√°cil", "Tema": "N√∫meros e opera√ß√µes", "Gab": "B"},
    148: {"TRI": 776.1, "H": "H21", "Nivel": "Muito Dif√≠cil", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "C"},
    149: {"TRI": 703.3, "H": "H14", "Nivel": "Intermedi√°rio", "Tema": "Grandezas e medidas", "Gab": "E"},
    150: {"TRI": 836.2, "H": "H13", "Nivel": "Muito Dif√≠cil", "Tema": "Grandezas e medidas", "Gab": "C"},
    151: {"TRI": 750.4, "H": "H11", "Nivel": "Muito Dif√≠cil", "Tema": "Geometria", "Gab": "E"},
    152: {"TRI": 604.0, "H": "H25", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "A"},
    153: {"TRI": 622.8, "H": "H1", "Nivel": "F√°cil", "Tema": "N√∫meros e opera√ß√µes", "Gab": "D"},
    154: {"TRI": 564.5, "H": "H27", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "C"},
    155: {"TRI": 723.7, "H": "H9", "Nivel": "Dif√≠cil", "Tema": "Geometria", "Gab": "E"},
    156: {"TRI": 591.2, "H": "H19", "Nivel": "F√°cil", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "B"},
    157: {"TRI": 611.7, "H": "H23", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "C"},
    158: {"TRI": 643.6, "H": "H4", "Nivel": "F√°cil", "Tema": "N√∫meros e opera√ß√µes", "Gab": "C"},
    159: {"TRI": 678.4, "H": "H10", "Nivel": "Intermedi√°rio", "Tema": "Geometria", "Gab": "C"},
    160: {"TRI": 684.5, "H": "H16", "Nivel": "Intermedi√°rio", "Tema": "Grandezas e medidas", "Gab": "D"},
    161: {"TRI": 738.7, "H": "H15", "Nivel": "Dif√≠cil", "Tema": "Grandezas e medidas", "Gab": "B"},
    162: {"TRI": 760.8, "H": "H5", "Nivel": "Muito Dif√≠cil", "Tema": "N√∫meros e opera√ß√µes", "Gab": "A"},
    163: {"TRI": 729.8, "H": "H12", "Nivel": "Dif√≠cil", "Tema": "Grandezas e medidas", "Gab": "D"},
    164: {"TRI": 712.4, "H": "H8", "Nivel": "Intermedi√°rio", "Tema": "Geometria", "Gab": "C"},
    165: {"TRI": 786.9, "H": "H30", "Nivel": "Muito Dif√≠cil", "Tema": "An√°lise combinat√≥ria", "Gab": "B"},
    166: {"TRI": 673.6, "H": "H19", "Nivel": "Intermedi√°rio", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "E"},
    167: {"TRI": 701.9, "H": "H3", "Nivel": "Intermedi√°rio", "Tema": "N√∫meros e opera√ß√µes", "Gab": "D"},
    168: {"TRI": 625.9, "H": "H15", "Nivel": "F√°cil", "Tema": "Grandezas e medidas", "Gab": "A"},
    169: {"TRI": 772.7, "H": "H28", "Nivel": "Muito Dif√≠cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "E"},
    170: {"TRI": 729.4, "H": "H21", "Nivel": "Dif√≠cil", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "C"},
    171: {"TRI": 787.2, "H": "H22", "Nivel": "Muito Dif√≠cil", "Tema": "√Ålgebra e fun√ß√µes", "Gab": "A"},
    172: {"TRI": 673.5, "H": "H17", "Nivel": "Intermedi√°rio", "Tema": "Grandezas e medidas", "Gab": "D"},
    173: {"TRI": 647.1, "H": "H29", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "A"},
    174: {"TRI": 663.0, "H": "H6", "Nivel": "Intermedi√°rio", "Tema": "Geometria", "Gab": "C"},
    175: {"TRI": 693.9, "H": "H12", "Nivel": "Intermedi√°rio", "Tema": "Grandezas e medidas", "Gab": "D"},
    176: {"TRI": 645.1, "H": "H24", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "B"},
    177: {"TRI": 673.9, "H": "H25", "Nivel": "Intermedi√°rio", "Tema": "Estat√≠stica e probabilidade", "Gab": "C"},
    178: {"TRI": 573.5, "H": "H27", "Nivel": "F√°cil", "Tema": "Estat√≠stica e probabilidade", "Gab": "E"},
    179: {"TRI": 706.9, "H": "H16", "Nivel": "Intermedi√°rio", "Tema": "Grandezas e medidas", "Gab": "C"},
    180: {"TRI": 742.5, "H": "H2", "Nivel": "Dif√≠cil", "Tema": "N√∫meros e opera√ß√µes", "Gab": "E"},
}

# Prompt CoT oficial
PROMPT_COT = """Elabore uma explica√ß√£o passo-a-passo que possibilite responder a quest√£o de m√∫ltipla escolha abaixo. Apenas uma alternativa √© correta.

Encerre a explica√ß√£o com "Resposta: " seguido pela alternativa."""

# Few-shot examples (simplificados para o script)
FEW_SHOT_EXAMPLES = [
    {
        "question": "Uma pessoa comprou 3 produtos por R$ 15,00 cada. Qual foi o total gasto?",
        "alternatives": ["A. R$ 30,00", "B. R$ 45,00", "C. R$ 60,00", "D. R$ 75,00", "E. R$ 90,00"],
        "response": "Para calcular o total gasto, multiplico a quantidade de produtos pelo pre√ßo unit√°rio:\n3 √ó R$ 15,00 = R$ 45,00\n\nResposta: B"
    },
    {
        "question": "Se x + 5 = 12, qual o valor de x?",
        "alternatives": ["A. 5", "B. 6", "C. 7", "D. 8", "E. 17"],
        "response": "Para encontrar x, isolo a vari√°vel:\nx + 5 = 12\nx = 12 - 5\nx = 7\n\nResposta: C"
    },
    {
        "question": "Um ret√¢ngulo tem base 4 cm e altura 3 cm. Qual sua √°rea?",
        "alternatives": ["A. 7 cm¬≤", "B. 12 cm¬≤", "C. 14 cm¬≤", "D. 24 cm¬≤", "E. 36 cm¬≤"],
        "response": "A √°rea de um ret√¢ngulo √© calculada por: √Årea = base √ó altura\n√Årea = 4 √ó 3 = 12 cm¬≤\n\nResposta: B"
    }
]


# =============================================================================
# FUN√á√ïES AUXILIARES
# =============================================================================

def setup_api():
    """Configura a API da Maritaca."""
    api_key = os.environ.get('CURSORMINIMAC') or os.environ.get('MARITALK_API_SECRET_KEY')
    
    if not api_key:
        print("‚ùå Erro: Chave API n√£o encontrada")
        print("   Configure CURSORMINIMAC ou MARITALK_API_SECRET_KEY")
        sys.exit(1)
    
    # Detectar vers√£o do openai
    openai_version = int(openai.__version__.split('.')[0])
    
    if openai_version >= 1:
        client = openai.OpenAI(
            api_key=api_key,
            base_url="https://chat.maritaca.ai/api"
        )
        return client, "new"
    else:
        openai.api_key = api_key
        openai.api_base = "https://chat.maritaca.ai/api"
        return None, "old"


def extract_answer(response_text):
    """Extrai a resposta (A, B, C, D ou E) do texto do modelo."""
    import re
    
    # Procurar padr√£o "Resposta: X" ou "Resposta: X."
    patterns = [
        r'[Rr]esposta:\s*([A-Ea-e])',
        r'[Rr]esposta:\s*\(?([A-Ea-e])\)?',
        r'[Aa]lternativa:\s*([A-Ea-e])',
        r'[Aa]lternativa\s+([A-Ea-e])',
        r'\b([A-Ea-e])\s*[.)\]]?\s*$',  # √öltima letra A-E no final
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response_text)
        if match:
            return match.group(1).upper()
    
    # Se n√£o encontrou, procurar a √∫ltima ocorr√™ncia de A, B, C, D ou E
    letters = re.findall(r'\b([A-Ea-e])[.)\]]?\b', response_text)
    if letters:
        return letters[-1].upper()
    
    return None


def call_model(client, api_type, messages, max_retries=3):
    """Chama o modelo com retry."""
    for attempt in range(max_retries):
        try:
            if api_type == "new":
                response = client.chat.completions.create(
                    model="sabia-3",
                    messages=messages,
                    max_tokens=1500,
                    temperature=0.1  # Baixa temperatura para consist√™ncia
                )
                return response.choices[0].message.content
            else:
                response = openai.ChatCompletion.create(
                    model="sabia-3",
                    messages=messages,
                    max_tokens=1500,
                    temperature=0.1
                )
                return response.choices[0].message.content
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Tentativa {attempt + 1} falhou: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            else:
                return None
    
    return None


def build_messages(question_data, use_captions=True):
    """Constr√≥i as mensagens para o modelo."""
    messages = [{"role": "system", "content": PROMPT_COT}]
    
    # Adicionar few-shot examples
    for example in FEW_SHOT_EXAMPLES:
        user_msg = f"Quest√£o: {example['question']}\n\nAlternativas:\n"
        user_msg += "\n".join(example['alternatives'])
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": example['response']})
    
    # Construir a quest√£o atual
    question_text = question_data['question']
    
    # Adicionar captions (descri√ß√µes de imagens) se dispon√≠veis
    if use_captions and question_data.get('description'):
        descriptions = question_data['description']
        if descriptions:
            if isinstance(descriptions, list):
                for i, desc in enumerate(descriptions):
                    question_text += f"\n\n[Descri√ß√£o da imagem {i+1}]: {desc}"
            else:
                question_text += f"\n\n[Descri√ß√£o da imagem]: {descriptions}"
    
    # Formatar alternativas
    alternatives = question_data['alternatives']
    alt_text = ""
    for i, alt in enumerate(alternatives):
        letter = chr(65 + i)  # A, B, C, D, E
        alt_text += f"{letter}. {alt}\n"
    
    user_msg = f"Quest√£o: {question_text}\n\nAlternativas:\n{alt_text}"
    messages.append({"role": "user", "content": user_msg})
    
    return messages


def load_enem_data():
    """Carrega os dados do ENEM 2024."""
    print("üì• Carregando dataset do ENEM 2024...")
    ds = load_dataset('maritaca-ai/enem', split='train')
    
    # Converter para lista
    questions = []
    for q in ds:
        q_num = int(q['id'].replace('questao_', ''))
        questions.append({
            'id': q['id'],
            'numero': q_num,
            'question': q['question'],
            'alternatives': q['alternatives'],
            'label': q['label'],
            'figures': q.get('figures', []),
            'description': q.get('description', []),
            'area': get_area(q_num)
        })
    
    return questions


def get_area(q_num):
    """Retorna a √°rea de conhecimento baseada no n√∫mero da quest√£o."""
    if 1 <= q_num <= 45:
        return "languages"
    elif 46 <= q_num <= 90:
        return "human-sciences"
    elif 91 <= q_num <= 135:
        return "natural-sciences"
    elif 136 <= q_num <= 180:
        return "mathematics"
    return "unknown"


# =============================================================================
# FUN√á√ÉO PRINCIPAL DE AVALIA√á√ÉO
# =============================================================================

def evaluate_with_logging(area="matematica", limit=None, output_dir="results"):
    """Executa avalia√ß√£o com logging detalhado."""
    
    print("=" * 70)
    print("üìä AVALIA√á√ÉO COM LOGGING DETALHADO - ENEM 2024")
    print("=" * 70)
    print()
    
    # Setup
    client, api_type = setup_api()
    print(f"‚úÖ API configurada (vers√£o: {api_type})")
    
    # Carregar dados
    questions = load_enem_data()
    print(f"‚úÖ {len(questions)} quest√µes carregadas")
    
    # Filtrar por √°rea
    if area == "matematica":
        questions = [q for q in questions if q['area'] == 'mathematics']
        print(f"‚úÖ Filtrado para matem√°tica: {len(questions)} quest√µes")
    elif area != "todas":
        print(f"‚ö†Ô∏è  √Årea '{area}' n√£o reconhecida. Usando todas.")
    
    # Aplicar limite
    if limit:
        questions = questions[:limit]
        print(f"‚úÖ Limitado a {limit} quest√µes")
    
    print()
    print("-" * 70)
    print("üöÄ Iniciando avalia√ß√£o...")
    print("-" * 70)
    print()
    
    # Resultados
    results = []
    correct = 0
    total = 0
    
    # Estat√≠sticas por categoria
    stats_by_nivel = {"F√°cil": {"correct": 0, "total": 0},
                      "Intermedi√°rio": {"correct": 0, "total": 0},
                      "Dif√≠cil": {"correct": 0, "total": 0},
                      "Muito Dif√≠cil": {"correct": 0, "total": 0}}
    
    stats_by_tema = {}
    
    for i, q in enumerate(questions):
        q_num = q['numero']
        total += 1
        
        # Obter dados TRI
        tri_info = TRI_DATA.get(q_num, {})
        tri = tri_info.get('TRI', 'N/A')
        nivel = tri_info.get('Nivel', 'N/A')
        tema = tri_info.get('Tema', 'N/A')
        habilidade = tri_info.get('H', 'N/A')
        
        print(f"[{i+1}/{len(questions)}] Quest√£o {q_num} (TRI: {tri}, {nivel})")
        
        # Construir mensagens e chamar modelo
        messages = build_messages(q, use_captions=True)
        response = call_model(client, api_type, messages)
        
        if response is None:
            print(f"   ‚ùå Erro ao obter resposta")
            model_answer = None
            is_correct = False
        else:
            model_answer = extract_answer(response)
            correct_answer = q['label']
            is_correct = (model_answer == correct_answer)
            
            if is_correct:
                correct += 1
                print(f"   ‚úÖ Correto! (Modelo: {model_answer}, Gabarito: {correct_answer})")
            else:
                print(f"   ‚ùå Errado! (Modelo: {model_answer}, Gabarito: {correct_answer})")
        
        # Atualizar estat√≠sticas
        if nivel in stats_by_nivel:
            stats_by_nivel[nivel]['total'] += 1
            if is_correct:
                stats_by_nivel[nivel]['correct'] += 1
        
        if tema not in stats_by_tema:
            stats_by_tema[tema] = {'correct': 0, 'total': 0}
        stats_by_tema[tema]['total'] += 1
        if is_correct:
            stats_by_tema[tema]['correct'] += 1
        
        # Salvar resultado detalhado
        result = {
            'questao': q_num,
            'id': q['id'],
            'tri': tri,
            'nivel': nivel,
            'tema': tema,
            'habilidade': habilidade,
            'gabarito': q['label'],
            'resposta_modelo': model_answer,
            'correto': is_correct,
            'resposta_completa': response[:500] if response else None,  # Truncar para economizar espa√ßo
            'questao_texto': q['question'][:300],  # Truncar
            'tem_figura': len(q.get('figures', [])) > 0,
            'tem_descricao': len(q.get('description', [])) > 0
        }
        results.append(result)
        
        # Pequena pausa para n√£o sobrecarregar a API
        time.sleep(0.5)
    
    # Calcular m√©tricas finais
    accuracy = correct / total if total > 0 else 0
    
    print()
    print("=" * 70)
    print("üìä RESULTADOS FINAIS")
    print("=" * 70)
    print()
    print(f"üìà Acur√°cia Geral: {accuracy:.2%} ({correct}/{total})")
    print()
    
    # Resultados por n√≠vel
    print("üìä Por N√≠vel de Dificuldade:")
    print("-" * 50)
    for nivel in ['F√°cil', 'Intermedi√°rio', 'Dif√≠cil', 'Muito Dif√≠cil']:
        stats = stats_by_nivel[nivel]
        if stats['total'] > 0:
            acc = stats['correct'] / stats['total']
            print(f"   {nivel:15} | {acc:6.1%} ({stats['correct']}/{stats['total']})")
    
    print()
    print("üìä Por Tema:")
    print("-" * 50)
    for tema, stats in sorted(stats_by_tema.items(), key=lambda x: -x[1]['total']):
        if stats['total'] > 0:
            acc = stats['correct'] / stats['total']
            print(f"   {tema:25} | {acc:6.1%} ({stats['correct']}/{stats['total']})")
    
    # Listar erros
    erros = [r for r in results if not r['correto']]
    print()
    print(f"‚ùå QUEST√ïES ERRADAS ({len(erros)}):")
    print("-" * 70)
    for erro in erros:
        print(f"   Q{erro['questao']} | TRI: {erro['tri']:>6} | {erro['nivel']:15} | {erro['tema']}")
        print(f"      Gabarito: {erro['gabarito']} | Modelo: {erro['resposta_modelo']}")
        print()
    
    # Salvar resultados
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"{output_dir}/avaliacao_detalhada_{timestamp}.json"
    
    output_data = {
        'config': {
            'area': area,
            'limit': limit,
            'timestamp': timestamp,
            'model': 'sabia-3',
            'num_fewshot': 3,
            'use_captions': True
        },
        'metricas': {
            'accuracy': accuracy,
            'correct': correct,
            'total': total,
            'por_nivel': {k: {'accuracy': v['correct']/v['total'] if v['total'] > 0 else 0, **v} 
                        for k, v in stats_by_nivel.items()},
            'por_tema': {k: {'accuracy': v['correct']/v['total'] if v['total'] > 0 else 0, **v} 
                        for k, v in stats_by_tema.items()}
        },
        'resultados': results,
        'erros': erros
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print()
    print(f"‚úÖ Resultados salvos em: {output_file}")
    
    # Salvar tamb√©m um CSV para f√°cil an√°lise
    csv_file = f"{output_dir}/erros_matematica_{timestamp}.csv"
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write("Questao,TRI,Nivel,Tema,Habilidade,Gabarito,Resposta_Modelo,Tem_Figura\n")
        for erro in erros:
            f.write(f"{erro['questao']},{erro['tri']},{erro['nivel']},{erro['tema']},"
                   f"{erro['habilidade']},{erro['gabarito']},{erro['resposta_modelo']},"
                   f"{erro['tem_figura']}\n")
    
    print(f"‚úÖ CSV de erros salvo em: {csv_file}")
    
    return output_data


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Avalia√ß√£o ENEM com logging detalhado")
    parser.add_argument("--area", type=str, default="matematica",
                       choices=["matematica", "todas"],
                       help="√Årea a avaliar (padr√£o: matematica)")
    parser.add_argument("--limit", type=int, default=None,
                       help="Limitar n√∫mero de quest√µes (para teste)")
    parser.add_argument("--output-dir", type=str, default="results",
                       help="Diret√≥rio de sa√≠da")
    
    args = parser.parse_args()
    
    evaluate_with_logging(
        area=args.area,
        limit=args.limit,
        output_dir=args.output_dir
    )

